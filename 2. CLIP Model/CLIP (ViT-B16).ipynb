{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a728816-8439-4744-b0ef-f71834902406",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3a728816-8439-4744-b0ef-f71834902406",
    "outputId": "170795b7-ef70-4ae5-ebc3-8217cc6cb00c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 28 15:50:45 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   33C    P0    46W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff84dfc3-14b3-4e9c-a84e-45d220bc57d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ff84dfc3-14b3-4e9c-a84e-45d220bc57d6",
    "outputId": "ea0e4d59-a234-4ef9-e6bf-fcad1e903e78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "! unzip -q '/content/drive/MyDrive/hateful_memes.zip' -d '/content/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcd350b4-79f8-4b8b-8d27-e58893fca8f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fcd350b4-79f8-4b8b-8d27-e58893fca8f5",
    "outputId": "4f9ff49c-4e29-44f3-f5f8-5d29eef03cdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "! pip install --quiet ftfy regex tqdm\n",
    "! pip install --quiet git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7588592-aa07-4e30-814b-adf3f7094b4b",
   "metadata": {
    "id": "f7588592-aa07-4e30-814b-adf3f7094b4b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.image as img\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "import cv2\n",
    "import clip\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "CcaiFYZFpTj9",
   "metadata": {
    "id": "CcaiFYZFpTj9"
   },
   "outputs": [],
   "source": [
    "class ScaleMaxSideToSize(object):\n",
    "  def __init__(self, size):\n",
    "    self.size = size\n",
    "\n",
    "  def __call__(self, sample):\n",
    "    sample = cv2.resize(sample, (self.size, self.size), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    return sample\n",
    "\n",
    "\n",
    "class CropCenter(object):\n",
    "  def __init__(self, size):\n",
    "    self.size = size\n",
    "\n",
    "  def __call__(self, sample):\n",
    "    img = sample\n",
    "    h, w, _ = img.shape\n",
    "    margin_h = (h - self.size) // 2\n",
    "    margin_w = (w - self.size) // 2\n",
    "    sample = img[margin_h:margin_h + self.size, margin_w:margin_w + self.size]\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8c9bb2-0949-422c-9d30-7069ad357881",
   "metadata": {
    "id": "7a8c9bb2-0949-422c-9d30-7069ad357881"
   },
   "source": [
    "## 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c106a57e-b40b-4769-8b23-3c55d76092c3",
   "metadata": {
    "id": "c106a57e-b40b-4769-8b23-3c55d76092c3"
   },
   "outputs": [],
   "source": [
    "class Load_Dataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, data_path, transforms):\n",
    "    self.data = [json.loads(l) for l in open(data_path)]\n",
    "    self.data_dir = os.path.dirname(data_path)\n",
    "    self.transforms = transforms\n",
    "\n",
    "  def __getitem__(self, index: int):\n",
    "    path = os.path.join(self.data_dir, self.data[index][\"img\"])\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    text = self.data[index][\"text\"]\n",
    "    label = self.data[index][\"label\"]\n",
    "\n",
    "    if self.transforms is not None:\n",
    "        image = self.transforms(image)\n",
    "\n",
    "    return image, text, label\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd0066bd-8704-4da6-a3b7-20b93ce65864",
   "metadata": {
    "id": "dd0066bd-8704-4da6-a3b7-20b93ce65864"
   },
   "outputs": [],
   "source": [
    "CROP_SIZE = 224\n",
    "MEAN = torch.tensor([0.485, 0.456, 0.406])\n",
    "STD = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "transforms = T.Compose([\n",
    "    ScaleMaxSideToSize(CROP_SIZE),\n",
    "    CropCenter(CROP_SIZE),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=MEAN, std=STD)])\n",
    "\n",
    "\n",
    "train_path = '/content/data/hateful_memes/train.jsonl'\n",
    "train_dataset = Load_Dataset(train_path, transforms)\n",
    "\n",
    "dev_path = '/content/data/hateful_memes/dev_seen.jsonl'\n",
    "dev_dataset = Load_Dataset(dev_path, transforms)\n",
    "\n",
    "test_path = '/content/data/hateful_memes/test_seen.jsonl'\n",
    "test_dataset = Load_Dataset(test_path, transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4b17f6-8ed7-4d2f-bcef-264d36e4acbf",
   "metadata": {
    "id": "ce4b17f6-8ed7-4d2f-bcef-264d36e4acbf"
   },
   "source": [
    "## 2. Use CLIP to encode each modality to get image and text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1af2651d-6f91-4eaf-8e67-5c0b55818a49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1af2651d-6f91-4eaf-8e67-5c0b55818a49",
    "outputId": "5481b343-a92f-4fe3-9f8b-bcde71362005"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 335M/335M [00:05<00:00, 68.0MiB/s]\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CLIP_model, preprocess = clip.load('ViT-B/16', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cc1a8ce-457e-4d75-8524-75f4612d84ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7cc1a8ce-457e-4d75-8524-75f4612d84ad",
    "outputId": "54696bb2-fe4f-43bd-e2ff-4b55321ddfba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85/85 [02:11<00:00,  1.54s/it]\n",
      "100%|██████████| 5/5 [00:07<00:00,  1.53s/it]\n",
      "100%|██████████| 10/10 [00:14<00:00,  1.48s/it]\n"
     ]
    }
   ],
   "source": [
    "def encode_features(model, dataset):\n",
    "  all_features = []\n",
    "  all_labels = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for images, texts, labels in tqdm(DataLoader(dataset, batch_size=100)):\n",
    "      image_input = torch.tensor(np.stack(images)).to(device)\n",
    "      text_tokens = clip.tokenize([desc[:77] for desc in texts]).to(device)\n",
    "\n",
    "      image_features = model.encode_image(image_input).type(torch.float).to(device)\n",
    "      text_features = model.encode_text(text_tokens).type(torch.float).to(device)\n",
    "\n",
    "      features = torch.cat([image_features, text_features], dim=1)\n",
    "\n",
    "      all_features.extend(features)\n",
    "      all_labels.extend(labels)\n",
    "\n",
    "  return all_features, all_labels\n",
    "\n",
    "\n",
    "# Use CLIP to encode each modality to get image & text features\n",
    "features_train, labels_train = encode_features(CLIP_model, train_dataset)\n",
    "features_dev, labels_dev = encode_features(CLIP_model, dev_dataset)\n",
    "features_test, labels_test = encode_features(CLIP_model, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "399d9c37-8491-4788-aadc-256b32910391",
   "metadata": {
    "id": "399d9c37-8491-4788-aadc-256b32910391"
   },
   "outputs": [],
   "source": [
    "class DefineDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, features, labels):\n",
    "    self.features = features\n",
    "    self.labels = labels\n",
    "\n",
    "  def __getitem__(self, index: int):\n",
    "    return self.features[index], self.labels[index]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.features)\n",
    "\n",
    "\n",
    "# Define train/dev/test set using image & text features and gold labels\n",
    "train_set = DefineDataset(features_train, labels_train)\n",
    "dev_set = DefineDataset(features_dev, labels_dev)\n",
    "test_set = DefineDataset(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e07bb92-4639-433e-a984-84170c85b9b8",
   "metadata": {
    "id": "5e07bb92-4639-433e-a984-84170c85b9b8"
   },
   "source": [
    "## 3. Construct a neural network for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4668dcc-54b2-4036-861b-2fe3849841fb",
   "metadata": {
    "id": "f4668dcc-54b2-4036-861b-2fe3849841fb"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(515)\n",
    "input_shape = features_train[0].shape[0]\n",
    "num_classes = 2\n",
    "shape = 256\n",
    "\n",
    "nn_model = nn.Sequential(\n",
    "    nn.Linear(input_shape, shape),\n",
    "    nn.Dropout(0.66),\n",
    "    nn.BatchNorm1d(shape),\n",
    "    nn.ReLU(inplace=True),\n",
    "\n",
    "    nn.Linear(shape, shape),\n",
    "    nn.Dropout(0.66),\n",
    "    nn.BatchNorm1d(shape),\n",
    "    nn.ReLU(inplace=True),\n",
    "\n",
    "    nn.Linear(shape, num_classes),)\n",
    "\n",
    "nn_model = nn_model.to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(nn_model.parameters(), lr=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325c07d5-2902-46ed-b60d-36ca93543c34",
   "metadata": {
    "id": "325c07d5-2902-46ed-b60d-36ca93543c34"
   },
   "source": [
    "## 4. Train and validate the classification model and save the best model after 100 epochs of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7700aaa-c37c-4b18-a30b-8e1df1653d51",
   "metadata": {
    "id": "b7700aaa-c37c-4b18-a30b-8e1df1653d51"
   },
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "  for param_group in optimizer.param_groups:\n",
    "    return param_group['lr']  # Retrieve the learning rate value from optimizer\n",
    "\n",
    "def compute_auroc(model, loader):\n",
    "  \"\"\"\n",
    "  Compute AUROC on the dataset wrapped in a loader\n",
    "  Return: AUROC score as a float value between 0 and 1\n",
    "  \"\"\"\n",
    "  model.eval()\n",
    "  real_labels = []\n",
    "  probabilities = []\n",
    "\n",
    "  for i_step, (x, y) in enumerate(loader):\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    prediction = model(x)\n",
    "    # select probabilities corresponding to the positive class\n",
    "    prediction = prediction[:, 1]  # positive class in the second column\n",
    "    probabilities.extend(prediction.detach().cpu().numpy())\n",
    "    real_labels.extend(y.detach().cpu().numpy())\n",
    "\n",
    "  auroc = roc_auc_score(real_labels, probabilities)*100\n",
    "  return auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e602c43-d58b-490a-b7d4-0e8c1190d93b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9e602c43-d58b-490a-b7d4-0e8c1190d93b",
    "outputId": "8ac38c54-2442-45f3-d37a-b4fd76a412a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 lr: 0.010000; Average loss: 0.624319, Val AUROC: 68.257829\n",
      "Epoch: 1 lr: 0.010000; Average loss: 0.518600, Val AUROC: 68.397049\n",
      "Epoch: 2 lr: 0.010000; Average loss: 0.462362, Val AUROC: 66.889632\n",
      "Epoch: 3 lr: 0.010000; Average loss: 0.447120, Val AUROC: 67.940984\n",
      "Epoch: 4 lr: 0.010000; Average loss: 0.424287, Val AUROC: 68.069002\n",
      "Epoch: 5 lr: 0.010000; Average loss: 0.411771, Val AUROC: 69.898065\n",
      "Epoch: 6 lr: 0.010000; Average loss: 0.389252, Val AUROC: 70.355731\n",
      "saved Best_model_70.36.ckpt\n",
      "Epoch: 7 lr: 0.010000; Average loss: 0.372925, Val AUROC: 71.615113\n",
      "saved Best_model_71.62.ckpt\n",
      "Epoch: 8 lr: 0.010000; Average loss: 0.356548, Val AUROC: 71.613512\n",
      "Epoch: 9 lr: 0.010000; Average loss: 0.341597, Val AUROC: 71.277464\n",
      "Epoch: 10 lr: 0.008000; Average loss: 0.321083, Val AUROC: 72.733674\n",
      "saved Best_model_72.73.ckpt\n",
      "Epoch: 11 lr: 0.008000; Average loss: 0.304476, Val AUROC: 72.904898\n",
      "saved Best_model_72.9.ckpt\n",
      "Epoch: 12 lr: 0.008000; Average loss: 0.283254, Val AUROC: 73.282553\n",
      "saved Best_model_73.28.ckpt\n",
      "Epoch: 13 lr: 0.008000; Average loss: 0.270457, Val AUROC: 72.381623\n",
      "Epoch: 14 lr: 0.008000; Average loss: 0.265288, Val AUROC: 73.165736\n",
      "Epoch: 15 lr: 0.008000; Average loss: 0.238019, Val AUROC: 73.292154\n",
      "saved Best_model_73.29.ckpt\n",
      "Epoch: 16 lr: 0.008000; Average loss: 0.238932, Val AUROC: 72.815285\n",
      "Epoch: 17 lr: 0.008000; Average loss: 0.222559, Val AUROC: 72.367221\n",
      "Epoch: 18 lr: 0.008000; Average loss: 0.213162, Val AUROC: 72.488838\n",
      "Epoch: 19 lr: 0.008000; Average loss: 0.199916, Val AUROC: 72.287209\n",
      "Epoch: 20 lr: 0.006400; Average loss: 0.188444, Val AUROC: 72.802484\n",
      "Epoch: 21 lr: 0.006400; Average loss: 0.169691, Val AUROC: 73.716215\n",
      "saved Best_model_73.72.ckpt\n",
      "Epoch: 22 lr: 0.006400; Average loss: 0.167799, Val AUROC: 73.120929\n",
      "Epoch: 23 lr: 0.006400; Average loss: 0.157650, Val AUROC: 73.364164\n",
      "Epoch: 24 lr: 0.006400; Average loss: 0.145347, Val AUROC: 73.444176\n",
      "Epoch: 25 lr: 0.006400; Average loss: 0.135107, Val AUROC: 73.404170\n",
      "Epoch: 26 lr: 0.006400; Average loss: 0.145276, Val AUROC: 74.133875\n",
      "saved Best_model_74.13.ckpt\n",
      "Epoch: 27 lr: 0.006400; Average loss: 0.130290, Val AUROC: 73.088925\n",
      "Epoch: 28 lr: 0.006400; Average loss: 0.136308, Val AUROC: 73.612200\n",
      "Epoch: 29 lr: 0.006400; Average loss: 0.129390, Val AUROC: 73.484182\n",
      "Epoch: 30 lr: 0.005120; Average loss: 0.112516, Val AUROC: 73.583396\n",
      "Epoch: 31 lr: 0.005120; Average loss: 0.120897, Val AUROC: 73.100126\n",
      "Epoch: 32 lr: 0.005120; Average loss: 0.100036, Val AUROC: 73.116129\n",
      "Epoch: 33 lr: 0.005120; Average loss: 0.103347, Val AUROC: 74.025060\n",
      "Epoch: 34 lr: 0.005120; Average loss: 0.094428, Val AUROC: 73.180138\n",
      "Epoch: 35 lr: 0.005120; Average loss: 0.090394, Val AUROC: 73.713015\n",
      "Epoch: 36 lr: 0.005120; Average loss: 0.088197, Val AUROC: 72.680866\n",
      "Epoch: 37 lr: 0.005120; Average loss: 0.092319, Val AUROC: 72.792882\n",
      "Epoch: 38 lr: 0.005120; Average loss: 0.075558, Val AUROC: 72.818486\n",
      "Epoch: 39 lr: 0.005120; Average loss: 0.077320, Val AUROC: 73.119329\n",
      "Epoch: 40 lr: 0.004096; Average loss: 0.076319, Val AUROC: 72.757677\n",
      "Epoch: 41 lr: 0.004096; Average loss: 0.071846, Val AUROC: 72.528844\n",
      "Epoch: 42 lr: 0.004096; Average loss: 0.070189, Val AUROC: 72.924101\n",
      "Epoch: 43 lr: 0.004096; Average loss: 0.060360, Val AUROC: 72.919300\n",
      "Epoch: 44 lr: 0.004096; Average loss: 0.063703, Val AUROC: 72.936903\n",
      "Epoch: 45 lr: 0.004096; Average loss: 0.057070, Val AUROC: 72.807284\n",
      "Epoch: 46 lr: 0.004096; Average loss: 0.055997, Val AUROC: 73.034517\n",
      "Epoch: 47 lr: 0.004096; Average loss: 0.062778, Val AUROC: 72.471236\n",
      "Epoch: 48 lr: 0.004096; Average loss: 0.057686, Val AUROC: 73.096926\n",
      "Epoch: 49 lr: 0.004096; Average loss: 0.057621, Val AUROC: 73.268151\n",
      "Epoch: 50 lr: 0.003277; Average loss: 0.057141, Val AUROC: 73.095326\n",
      "Epoch: 51 lr: 0.003277; Average loss: 0.053611, Val AUROC: 73.805828\n",
      "Epoch: 52 lr: 0.003277; Average loss: 0.044992, Val AUROC: 73.849034\n",
      "Epoch: 53 lr: 0.003277; Average loss: 0.038447, Val AUROC: 73.405770\n",
      "Epoch: 54 lr: 0.003277; Average loss: 0.044596, Val AUROC: 73.613800\n",
      "Epoch: 55 lr: 0.003277; Average loss: 0.046460, Val AUROC: 73.527388\n",
      "Epoch: 56 lr: 0.003277; Average loss: 0.038933, Val AUROC: 73.721016\n",
      "Epoch: 57 lr: 0.003277; Average loss: 0.041082, Val AUROC: 73.554592\n",
      "Epoch: 58 lr: 0.003277; Average loss: 0.043566, Val AUROC: 73.705014\n",
      "Epoch: 59 lr: 0.003277; Average loss: 0.037785, Val AUROC: 73.463379\n",
      "Epoch: 60 lr: 0.002621; Average loss: 0.041403, Val AUROC: 73.498584\n",
      "Epoch: 61 lr: 0.002621; Average loss: 0.038359, Val AUROC: 73.300155\n",
      "Epoch: 62 lr: 0.002621; Average loss: 0.033288, Val AUROC: 73.533789\n",
      "Epoch: 63 lr: 0.002621; Average loss: 0.033814, Val AUROC: 73.751420\n",
      "Epoch: 64 lr: 0.002621; Average loss: 0.035805, Val AUROC: 73.946648\n",
      "Epoch: 65 lr: 0.002621; Average loss: 0.032591, Val AUROC: 73.412171\n",
      "Epoch: 66 lr: 0.002621; Average loss: 0.030853, Val AUROC: 73.276152\n",
      "Epoch: 67 lr: 0.002621; Average loss: 0.029906, Val AUROC: 73.544990\n",
      "Epoch: 68 lr: 0.002621; Average loss: 0.028215, Val AUROC: 73.738618\n",
      "Epoch: 69 lr: 0.002621; Average loss: 0.031141, Val AUROC: 73.663408\n",
      "Epoch: 70 lr: 0.002097; Average loss: 0.031982, Val AUROC: 73.868237\n",
      "Epoch: 71 lr: 0.002097; Average loss: 0.031228, Val AUROC: 73.645805\n",
      "Epoch: 72 lr: 0.002097; Average loss: 0.027506, Val AUROC: 73.429774\n",
      "Epoch: 73 lr: 0.002097; Average loss: 0.026911, Val AUROC: 73.738618\n",
      "Epoch: 74 lr: 0.002097; Average loss: 0.026155, Val AUROC: 73.879439\n",
      "Epoch: 75 lr: 0.002097; Average loss: 0.026739, Val AUROC: 73.761022\n",
      "Epoch: 76 lr: 0.002097; Average loss: 0.029068, Val AUROC: 74.226689\n",
      "saved Best_model_74.23.ckpt\n",
      "Epoch: 77 lr: 0.002097; Average loss: 0.026312, Val AUROC: 74.292298\n",
      "saved Best_model_74.29.ckpt\n",
      "Epoch: 78 lr: 0.002097; Average loss: 0.023863, Val AUROC: 73.839433\n",
      "Epoch: 79 lr: 0.002097; Average loss: 0.028004, Val AUROC: 73.873038\n",
      "Epoch: 80 lr: 0.001678; Average loss: 0.022023, Val AUROC: 73.925845\n",
      "Epoch: 81 lr: 0.001678; Average loss: 0.023271, Val AUROC: 73.930646\n",
      "Epoch: 82 lr: 0.001678; Average loss: 0.021402, Val AUROC: 74.018659\n",
      "Epoch: 83 lr: 0.001678; Average loss: 0.025198, Val AUROC: 74.050663\n",
      "Epoch: 84 lr: 0.001678; Average loss: 0.017090, Val AUROC: 74.129075\n",
      "Epoch: 85 lr: 0.001678; Average loss: 0.025005, Val AUROC: 73.764222\n",
      "Epoch: 86 lr: 0.001678; Average loss: 0.021054, Val AUROC: 73.858636\n",
      "Epoch: 87 lr: 0.001678; Average loss: 0.019907, Val AUROC: 73.692212\n",
      "Epoch: 88 lr: 0.001678; Average loss: 0.021595, Val AUROC: 73.469780\n",
      "Epoch: 89 lr: 0.001678; Average loss: 0.016995, Val AUROC: 73.572194\n",
      "Epoch: 90 lr: 0.001342; Average loss: 0.017244, Val AUROC: 73.434575\n",
      "Epoch: 91 lr: 0.001342; Average loss: 0.018699, Val AUROC: 73.746620\n",
      "Epoch: 92 lr: 0.001342; Average loss: 0.018934, Val AUROC: 73.708214\n",
      "Epoch: 93 lr: 0.001342; Average loss: 0.018798, Val AUROC: 73.549791\n",
      "Epoch: 94 lr: 0.001342; Average loss: 0.019121, Val AUROC: 73.677810\n",
      "Epoch: 95 lr: 0.001342; Average loss: 0.020089, Val AUROC: 73.586596\n",
      "Epoch: 96 lr: 0.001342; Average loss: 0.015824, Val AUROC: 73.833032\n",
      "Epoch: 97 lr: 0.001342; Average loss: 0.014503, Val AUROC: 74.137076\n",
      "Epoch: 98 lr: 0.001342; Average loss: 0.014658, Val AUROC: 73.988254\n",
      "Epoch: 99 lr: 0.001342; Average loss: 0.015455, Val AUROC: 74.036261\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=500)\n",
    "val_loader = DataLoader(dev_set, batch_size=500)\n",
    "top_val_AUROC = 70\n",
    "\n",
    "for epoch in range(100):\n",
    "  nn_model.train()\n",
    "  loss_accum = 0\n",
    "  for i_step, (x, y) in enumerate(train_loader):\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    prediction = nn_model(x)\n",
    "    loss_value = loss(prediction, y.type(torch.long))\n",
    "    optimizer.zero_grad()\n",
    "    loss_value.backward()\n",
    "    optimizer.step()\n",
    "    loss_accum += loss_value\n",
    "\n",
    "  ave_loss = loss_accum / (i_step + 1)\n",
    "  val_AUROC = compute_auroc(nn_model, val_loader)\n",
    "  print(\"Epoch: %i lr: %f; Average loss: %f, Val AUROC: %f\" % (epoch, get_lr(optimizer), ave_loss, val_AUROC))\n",
    "\n",
    "  if scheduler != None:\n",
    "    scheduler.step()\n",
    "\n",
    "  if val_AUROC > top_val_AUROC:\n",
    "    top_val_AUROC = val_AUROC\n",
    "    best_model_name = f'Best_model_{round(val_AUROC, 2)}.ckpt'\n",
    "    torch.save(nn_model, open(best_model_name, 'wb'))\n",
    "    print(\"saved\", best_model_name)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
