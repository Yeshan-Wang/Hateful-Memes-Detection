{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3a728816-8439-4744-b0ef-f71834902406",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a728816-8439-4744-b0ef-f71834902406",
        "outputId": "b1649bda-ecf5-4033-fe8c-5ebec25d49e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jul 19 21:47:07 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    48W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ff84dfc3-14b3-4e9c-a84e-45d220bc57d6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff84dfc3-14b3-4e9c-a84e-45d220bc57d6",
        "outputId": "5bdf3c11-8e30-4a54-be5a-d2d5dc2dd9aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "! unzip -q '/content/drive/MyDrive/hateful_memes.zip' -d '/content/data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fcd350b4-79f8-4b8b-8d27-e58893fca8f5",
      "metadata": {
        "id": "fcd350b4-79f8-4b8b-8d27-e58893fca8f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97d37f59-72c7-4674-a089-1b0c2762329b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "! pip install --quiet ftfy regex tqdm\n",
        "! pip install --quiet git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f7588592-aa07-4e30-814b-adf3f7094b4b",
      "metadata": {
        "id": "f7588592-aa07-4e30-814b-adf3f7094b4b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.image as img\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR, LinearLR\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms as T\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "import cv2\n",
        "import clip\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from sklearn.metrics import roc_auc_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Integrate image captioning features as another column to the dataset"
      ],
      "metadata": {
        "id": "ASvOKpKoKlHi"
      },
      "id": "ASvOKpKoKlHi"
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the caption information and store them in a dictionary using img path as key\n",
        "caption = {}\n",
        "with open('/content/drive/MyDrive/BLIP_2_caption.csv', 'r') as csvfile:\n",
        "  csvreader = csv.reader(csvfile)\n",
        "  for row in csvreader:\n",
        "    caption[row[0]] = row[1]\n",
        "\n",
        "file_path = ['/content/data/hateful_memes/train.jsonl',\n",
        "             '/content/data/hateful_memes/dev_seen.jsonl',\n",
        "             '/content/data/hateful_memes/test_seen.jsonl']\n",
        "\n",
        "for path in file_path:\n",
        "  # Add caption information as another column to the dataset\n",
        "  data = []\n",
        "  with open(path, 'r') as jsonfile:\n",
        "    for line in jsonfile:\n",
        "      data.append(json.loads(line))\n",
        "\n",
        "  combined_data = []\n",
        "  for item in data:\n",
        "    img = item['img']\n",
        "    combined_data.append({'id': item['id'], 'img': img, 'label': item['label'], 'text': item['text'], 'caption': caption[img]})\n",
        "\n",
        "  os.remove(path)\n",
        "  with open(path, 'w') as file:\n",
        "    for i in combined_data:\n",
        "      file.write(json.dumps(i) + '\\n')"
      ],
      "metadata": {
        "id": "w4qCI6QOLb8k"
      },
      "id": "w4qCI6QOLb8k",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7a8c9bb2-0949-422c-9d30-7069ad357881",
      "metadata": {
        "id": "7a8c9bb2-0949-422c-9d30-7069ad357881"
      },
      "source": [
        "## 2. Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaleMaxSideToSize(object):\n",
        "  def __init__(self, size):\n",
        "    self.size = size\n",
        "\n",
        "  def __call__(self, sample):\n",
        "    sample = cv2.resize(sample, (self.size, self.size), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    return sample\n",
        "\n",
        "\n",
        "class CropCenter(object):\n",
        "  def __init__(self, size):\n",
        "    self.size = size\n",
        "\n",
        "  def __call__(self, sample):\n",
        "    img = sample\n",
        "    h, w, _ = img.shape\n",
        "    margin_h = (h - self.size) // 2\n",
        "    margin_w = (w - self.size) // 2\n",
        "    sample = img[margin_h:margin_h + self.size, margin_w:margin_w + self.size]\n",
        "\n",
        "    return sample"
      ],
      "metadata": {
        "id": "CcaiFYZFpTj9"
      },
      "id": "CcaiFYZFpTj9",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c106a57e-b40b-4769-8b23-3c55d76092c3",
      "metadata": {
        "id": "c106a57e-b40b-4769-8b23-3c55d76092c3"
      },
      "outputs": [],
      "source": [
        "class Load_Dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, data_path, transforms):\n",
        "    self.data = [json.loads(l) for l in open(data_path)]\n",
        "    self.data_dir = os.path.dirname(data_path)\n",
        "    self.transforms = transforms\n",
        "\n",
        "  def __getitem__(self, index: int):\n",
        "    path = os.path.join(self.data_dir, self.data[index][\"img\"])\n",
        "    image = cv2.imread(path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    id = self.data[index][\"id\"]\n",
        "    text = self.data[index][\"text\"]\n",
        "    label = self.data[index][\"label\"]\n",
        "    caption = self.data[index][\"caption\"]\n",
        "\n",
        "    if self.transforms is not None:\n",
        "        image = self.transforms(image)\n",
        "\n",
        "    return id, image, text, label, caption\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "dd0066bd-8704-4da6-a3b7-20b93ce65864",
      "metadata": {
        "id": "dd0066bd-8704-4da6-a3b7-20b93ce65864"
      },
      "outputs": [],
      "source": [
        "CROP_SIZE = 224\n",
        "MEAN = torch.tensor([0.485, 0.456, 0.406])\n",
        "STD = torch.tensor([0.229, 0.224, 0.225])\n",
        "\n",
        "transforms = T.Compose([\n",
        "    ScaleMaxSideToSize(CROP_SIZE),\n",
        "    CropCenter(CROP_SIZE),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=MEAN, std=STD)])\n",
        "\n",
        "\n",
        "train_path = '/content/data/hateful_memes/train.jsonl'\n",
        "train_dataset = Load_Dataset(train_path, transforms)\n",
        "\n",
        "dev_path = '/content/data/hateful_memes/dev_seen.jsonl'\n",
        "dev_dataset = Load_Dataset(dev_path, transforms)\n",
        "\n",
        "test_path = '/content/data/hateful_memes/test_seen.jsonl'\n",
        "test_dataset = Load_Dataset(test_path, transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce4b17f6-8ed7-4d2f-bcef-264d36e4acbf",
      "metadata": {
        "id": "ce4b17f6-8ed7-4d2f-bcef-264d36e4acbf"
      },
      "source": [
        "## 3. Use CLIP to encode each modality to get image, text and caption features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1af2651d-6f91-4eaf-8e67-5c0b55818a49",
      "metadata": {
        "id": "1af2651d-6f91-4eaf-8e67-5c0b55818a49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caee6d69-8aac-47d8-d60b-3ffad3f5a87e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 890M/890M [00:06<00:00, 137MiB/s]\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "CLIP_model, preprocess = clip.load('ViT-L/14', device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "7cc1a8ce-457e-4d75-8524-75f4612d84ad",
      "metadata": {
        "id": "7cc1a8ce-457e-4d75-8524-75f4612d84ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f202800-3a9e-495e-edeb-6bb4d70c29bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85/85 [02:23<00:00,  1.69s/it]\n",
            "100%|██████████| 5/5 [00:08<00:00,  1.68s/it]\n",
            "100%|██████████| 10/10 [00:16<00:00,  1.64s/it]\n"
          ]
        }
      ],
      "source": [
        "def encode_features(model, dataset):\n",
        "  all_ids = []\n",
        "  all_features = []\n",
        "  all_labels = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for ids, images, texts, labels, caption in tqdm(DataLoader(dataset, batch_size=100)):\n",
        "      image_input = torch.tensor(np.stack(images)).to(device)\n",
        "      text_tokens = clip.tokenize([desc[:77] for desc in texts]).to(device)\n",
        "      caption_tokens = clip.tokenize([desc for desc in caption]).to(device)\n",
        "\n",
        "      image_features = model.encode_image(image_input).type(torch.float).to(device)\n",
        "      text_features = model.encode_text(text_tokens).type(torch.float).to(device)\n",
        "      caption_features = model.encode_text(caption_tokens).type(torch.float).to(device)\n",
        "\n",
        "      features = torch.cat([image_features, text_features, caption_features], dim=1)\n",
        "      all_ids.extend(ids)\n",
        "      all_features.extend(features)\n",
        "      all_labels.extend(labels)\n",
        "\n",
        "  return all_ids, all_features, all_labels\n",
        "\n",
        "\n",
        "# Use CLIP to encode each modality to get image, text and caption features\n",
        "ids_train, features_train, labels_train = encode_features(CLIP_model, train_dataset)\n",
        "ids_dev, features_dev, labels_dev = encode_features(CLIP_model, dev_dataset)\n",
        "ids_test, features_test, labels_test = encode_features(CLIP_model, test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "399d9c37-8491-4788-aadc-256b32910391",
      "metadata": {
        "id": "399d9c37-8491-4788-aadc-256b32910391"
      },
      "outputs": [],
      "source": [
        "class DefineDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, ids, features, labels):\n",
        "    self.ids = ids\n",
        "    self.features = features\n",
        "    self.labels = labels\n",
        "\n",
        "  def __getitem__(self, index: int):\n",
        "    return self.ids[index], self.features[index], self.labels[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.features)\n",
        "\n",
        "\n",
        "# Define train/dev/test set using image & text features and gold labels\n",
        "train_set = DefineDataset(ids_train, features_train, labels_train)\n",
        "dev_set = DefineDataset(ids_dev, features_dev, labels_dev)\n",
        "test_set = DefineDataset(ids_test, features_test, labels_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e07bb92-4639-433e-a984-84170c85b9b8",
      "metadata": {
        "id": "5e07bb92-4639-433e-a984-84170c85b9b8"
      },
      "source": [
        "## 4. Hhyper-parameters tuning for maximum epochs, learning rate, scheduler type, and save all best performing models based on the validation AUROC score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b7700aaa-c37c-4b18-a30b-8e1df1653d51",
      "metadata": {
        "id": "b7700aaa-c37c-4b18-a30b-8e1df1653d51"
      },
      "outputs": [],
      "source": [
        "def get_lr(optimizer):\n",
        "  for param_group in optimizer.param_groups:\n",
        "    return param_group['lr']  # Retrieve the learning rate value from optimizer\n",
        "\n",
        "def compute_auroc(model, loader):\n",
        "  \"\"\"\n",
        "  Compute AUROC on the dataset wrapped in a loader\n",
        "  Return: AUROC score as a float value between 0 and 1\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  real_labels = []\n",
        "  probabilities = []\n",
        "\n",
        "  for i_step, (i, x, y) in enumerate(loader):\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    prediction = model(x)\n",
        "    # select probabilities corresponding to the positive class\n",
        "    prediction = prediction[:, 1]  # positive class in the second column\n",
        "    probabilities.extend(prediction.detach().cpu().numpy())\n",
        "    real_labels.extend(y.detach().cpu().numpy())\n",
        "\n",
        "  auroc = roc_auc_score(real_labels, probabilities)*100\n",
        "  return auroc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = features_train[0].shape[0]\n",
        "num_classes = 2\n",
        "torch.manual_seed(78)\n",
        "shape = 256\n",
        "\n",
        "# Define hyperparameters\n",
        "dropout_rates = [0.6, 0.7]\n",
        "activation_functions = ['ReLU', 'LeakyReLU', 'ELU']\n",
        "optimizers = ['AdamW', 'Adam']\n",
        "learning_rates = [1e-2, 1e-3]\n",
        "schedulers = ['StepLR', 'LinearLR']\n",
        "top_val_AUROC = 81.79\n",
        "\n",
        "best_models = []\n",
        "\n",
        "for dropout_rate in dropout_rates:\n",
        "  for act_func in activation_functions:\n",
        "    for optimizer_name in optimizers:\n",
        "      for lr in learning_rates:\n",
        "        for sched in schedulers:\n",
        "\n",
        "          # Construct a neural network for classification\n",
        "          nn_model = nn.Sequential(\n",
        "              nn.Linear(input_shape, shape),\n",
        "              nn.Dropout(dropout_rate),\n",
        "              nn.BatchNorm1d(shape),\n",
        "              getattr(nn, act_func)(inplace=True),\n",
        "\n",
        "              nn.Linear(shape, shape),\n",
        "              nn.Dropout(dropout_rate),\n",
        "              nn.BatchNorm1d(shape),\n",
        "              getattr(nn, act_func)(inplace=True),\n",
        "\n",
        "              nn.Linear(shape, num_classes),)\n",
        "\n",
        "          nn_model = nn_model.to(device)\n",
        "          loss = nn.CrossEntropyLoss()\n",
        "          optimizer = torch.optim.Adam(nn_model.parameters(), lr=lr)\n",
        "\n",
        "          if optimizer_name == 'Adam':\n",
        "            optimizer = torch.optim.Adam(nn_model.parameters(), lr=lr)\n",
        "          if optimizer_name == 'AdamW':\n",
        "            optimizer = torch.optim.AdamW(nn_model.parameters(), lr=lr)\n",
        "\n",
        "          if sched == 'StepLR':\n",
        "            scheduler = StepLR(optimizer, step_size=10, gamma=0.8)\n",
        "          if sched == 'LinearLR':\n",
        "            scheduler = LinearLR(optimizer)\n",
        "\n",
        "          train_loader = DataLoader(train_set, batch_size=500)\n",
        "          val_loader = DataLoader(dev_set, batch_size=500)\n",
        "\n",
        "          for epoch in range(100):\n",
        "            nn_model.train()\n",
        "            loss_accum = 0\n",
        "            for i_step, (i, x, y) in enumerate(train_loader):\n",
        "              x = x.to(device)\n",
        "              y = y.to(device)\n",
        "              prediction = nn_model(x)\n",
        "              loss_value = loss(prediction, y.type(torch.long))\n",
        "              optimizer.zero_grad()\n",
        "              loss_value.backward()\n",
        "              optimizer.step()\n",
        "              loss_accum += loss_value\n",
        "\n",
        "            ave_loss = loss_accum / (i_step + 1)\n",
        "            val_AUROC = compute_auroc(nn_model, val_loader)\n",
        "            if scheduler != None:\n",
        "              scheduler.step()\n",
        "\n",
        "            # Save the best models based on validation AUROC\n",
        "            if val_AUROC > top_val_AUROC:\n",
        "              top_val_AUROC = val_AUROC\n",
        "              m_name = f'Epoch_{epoch}_Dropout_{dropout_rate}_Activation_{act_func}_Optimizer_{optimizer_name}_Learning_rate_{lr}_Scheduler_{sched}.ckpt'\n",
        "              torch.save(nn_model, open(m_name, 'wb'))\n",
        "              best_models.append(m_name)\n",
        "              print('Saved Model:', m_name, ' Validation AUROC:', round(val_AUROC, 2))"
      ],
      "metadata": {
        "id": "Wsho-GlHFWPY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52a5655a-d149-4707-a9e7-63a86857d448"
      },
      "id": "Wsho-GlHFWPY",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved Model: Epoch_57_Dropout_0.6_Activation_ReLU_Optimizer_AdamW_Learning_rate_0.01_Scheduler_StepLR.ckpt  Validation AUROC: 81.79\n",
            "Saved Model: Epoch_15_Dropout_0.6_Activation_ReLU_Optimizer_Adam_Learning_rate_0.01_Scheduler_StepLR.ckpt  Validation AUROC: 82.09\n",
            "Saved Model: Epoch_50_Dropout_0.6_Activation_ReLU_Optimizer_Adam_Learning_rate_0.01_Scheduler_LinearLR.ckpt  Validation AUROC: 82.16\n",
            "Saved Model: Epoch_83_Dropout_0.6_Activation_LeakyReLU_Optimizer_AdamW_Learning_rate_0.01_Scheduler_LinearLR.ckpt  Validation AUROC: 82.2\n",
            "Saved Model: Epoch_87_Dropout_0.6_Activation_LeakyReLU_Optimizer_AdamW_Learning_rate_0.01_Scheduler_LinearLR.ckpt  Validation AUROC: 82.33\n",
            "Saved Model: Epoch_68_Dropout_0.7_Activation_ReLU_Optimizer_AdamW_Learning_rate_0.01_Scheduler_LinearLR.ckpt  Validation AUROC: 82.45\n",
            "Saved Model: Epoch_50_Dropout_0.7_Activation_LeakyReLU_Optimizer_AdamW_Learning_rate_0.01_Scheduler_LinearLR.ckpt  Validation AUROC: 82.47\n",
            "Saved Model: Epoch_88_Dropout_0.7_Activation_LeakyReLU_Optimizer_AdamW_Learning_rate_0.01_Scheduler_LinearLR.ckpt  Validation AUROC: 82.56\n",
            "Saved Model: Epoch_91_Dropout_0.7_Activation_LeakyReLU_Optimizer_AdamW_Learning_rate_0.01_Scheduler_LinearLR.ckpt  Validation AUROC: 82.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "325c07d5-2902-46ed-b60d-36ca93543c34",
      "metadata": {
        "id": "325c07d5-2902-46ed-b60d-36ca93543c34"
      },
      "source": [
        "## 5. Perform soft voting method for ensemble learning by averaging the predictions of best performing models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define functions to calculate average predicted probabilities and labels from all models\n",
        "\n",
        "def calculate_average_proba(arrays_list):\n",
        "    num_arrays = len(arrays_list)\n",
        "    array_size = len(arrays_list[0])\n",
        "    averaged_proba = []\n",
        "\n",
        "    for i in range(array_size):\n",
        "        total = sum(arr[i] for arr in arrays_list)\n",
        "        avg = total / num_arrays\n",
        "        averaged_proba.append(avg)\n",
        "\n",
        "    return averaged_proba\n",
        "\n",
        "def calculate_average_label(arrays_list):\n",
        "    num_sublists = len(arrays_list)\n",
        "    sublist_size = len(arrays_list[0])\n",
        "    averaged_labels = []\n",
        "\n",
        "    for i in range(sublist_size):\n",
        "        total = sum(sublist[i] for sublist in arrays_list)\n",
        "        avg = total / num_sublists\n",
        "        avg_label = 1 if avg >= 0.5 else 0\n",
        "        averaged_labels.append(avg_label)\n",
        "\n",
        "    return averaged_labels"
      ],
      "metadata": {
        "id": "gQ0SuPOfkT1Z"
      },
      "id": "gQ0SuPOfkT1Z",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = [] # Load best performing models and store them in a list\n",
        "for best_model_name in best_models:\n",
        "  best_model = torch.load(open(best_model_name, 'rb'))\n",
        "  best_model = best_model.to(device)\n",
        "  best_model.eval()\n",
        "  models.append(best_model)"
      ],
      "metadata": {
        "id": "SvbWiFz_mDrs"
      },
      "id": "SvbWiFz_mDrs",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader = DataLoader(dev_set, batch_size=500)\n",
        "val_real_label = []\n",
        "val_pred_label = []\n",
        "val_pred_proba = []\n",
        "\n",
        "for i_step, (i, x, y) in enumerate(val_loader):\n",
        "  x = x.to(device)\n",
        "  y = y.to(device)\n",
        "  val_real_label.extend(y.detach().cpu().numpy())\n",
        "\n",
        "  # Get predicted probabilities and labels from each model\n",
        "  for model in models:\n",
        "    prediction = model(x)\n",
        "    val_pred_proba.append(prediction[:, 1].detach().cpu().numpy())\n",
        "    val_pred_label.append(torch.max(prediction.cpu(), 1)[1])\n",
        "\n",
        "# Calculate average predicted probabilities and labels from all models\n",
        "val_ensemble_proba = calculate_average_proba(val_pred_proba)\n",
        "val_ensemble_label = calculate_average_label(val_pred_label)\n",
        "\n",
        "# Compute the AUROC score for the ensemble predictions on validation set\n",
        "auroc_score = roc_auc_score(val_real_label, val_ensemble_proba)*100\n",
        "print('Validation AUROC:', round(auroc_score, 2))"
      ],
      "metadata": {
        "id": "iiZNBBztS4_M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e4fc557-cde4-48cb-c263-a6fa6b7e6ba4"
      },
      "id": "iiZNBBztS4_M",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation AUROC: 83.46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = DataLoader(test_set, batch_size=1000)\n",
        "test_real_label = []\n",
        "test_pred_label = []\n",
        "test_pred_proba = []\n",
        "\n",
        "for i_step, (i, x, y) in enumerate(test_loader):\n",
        "  x = x.to(device)\n",
        "  y = y.to(device)\n",
        "  test_real_label.extend(y.detach().cpu().numpy())\n",
        "\n",
        "  # Get predicted probabilities and labels from each model\n",
        "  for model in models:\n",
        "    prediction = model(x)\n",
        "    test_pred_proba.append(prediction[:, 1].detach().cpu().numpy())\n",
        "    test_pred_label.append(torch.max(prediction.cpu(), 1)[1])\n",
        "\n",
        "# Calculate average predicted probabilities and labels from all models\n",
        "test_ensemble_proba = calculate_average_proba(test_pred_proba)\n",
        "test_ensemble_label = calculate_average_label(test_pred_label)\n",
        "\n",
        "# Compute the AUROC score for the ensemble predictions on test set\n",
        "auroc_score = roc_auc_score(test_real_label, test_ensemble_proba)*100\n",
        "print('Test AUROC:', round(auroc_score, 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piEhj8-FybRD",
        "outputId": "65bd42c1-c0fc-437e-c02e-5364631b3a31"
      },
      "id": "piEhj8-FybRD",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test AUROC: 83.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the prediction results as a csv file\n",
        "data = {'id': ids_test, 'proba': test_ensemble_proba, 'label': test_ensemble_label}\n",
        "results = pd.DataFrame(data)\n",
        "file_path = \"/content/data/CLIP_ensemble.csv\"\n",
        "results.to_csv(file_path, index=False)\n",
        "\n",
        "# Save the csv file in Google Drive\n",
        "!cp -r /content/data/CLIP_ensemble.csv /content/drive/MyDrive/results"
      ],
      "metadata": {
        "id": "OE3-pCxS0eUS"
      },
      "id": "OE3-pCxS0eUS",
      "execution_count": 18,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}